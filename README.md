# Quantum vs Classical Machine Learning ‚Äî A Comparative Study

> **Benchmarking Variational Quantum Circuits against Classical ML/DL on the Iris Dataset**

Based on the foundational paper: *"Training a Quantum Neural Network"* ‚Äî Ventura & Martinez, NIPS 2003

---

## üìã Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
  - [Classical Models](#classical-models)
  - [Variational Quantum Classifier (VQC)](#variational-quantum-classifier-vqc)
- [Tech Stack](#tech-stack)
- [Setup & Installation](#setup--installation)
- [Usage](#usage)
- [Results & Comparison](#results--comparison)
- [References](#references)

---

## Overview

This project presents a **rigorous comparative analysis** between classical machine learning models and quantum neural network architectures for classification tasks. We implement and evaluate five distinct models on the same datasets under identical experimental conditions to determine whether quantum approaches can achieve competitive performance against well-established classical methods.

### Models Implemented

| # | Model | Type | Framework |
|---|-------|------|-----------|
| 1 | **Support Vector Machine** (RBF Kernel) | Classical ML | scikit-learn |
| 2 | **Feedforward Neural Network** | Classical DL | PyTorch |
| 3 | **Random Forest** | Classical ML (Ensemble) | scikit-learn |
| 4 | **Variational Quantum Classifier (VQC)** | Quantum ML | PennyLane |

---

## Architecture

### Classical Models

**SVM (RBF Kernel):** Hyperparameter-tuned via 5-fold GridSearchCV over `C ‚àà {0.1, 1, 10, 100}` and `Œ≥ ‚àà {scale, auto, 0.01, 0.001}`. Uses all available features with StandardScaler normalization.

**Artificial Neural Network:** PyTorch feedforward network with architecture `Input ‚Üí 64 ‚Üí 32 ‚Üí 16 ‚Üí Output`, using ReLU activations, BatchNorm, Dropout (0.2), and Adam optimizer with learning rate scheduling.

---


#### VQ Circuit Architecture

| Stage | Gates | Purpose |
|-------|-------|---------|
| **Angle Embedding** | RX(x·µ¢) on each qubit | Encode classical features as quantum rotation angles |
| **Variational Layer** (√ó3) | RY(Œ∏) + RZ(œÜ) per qubit | Trainable parameterized rotations |
| **Entanglement** (√ó3) | CNOT ring topology | Create quantum correlations between qubits |
| **Measurement** | ‚ü®Z‚ü© on qubits 0‚Äì2 | Extract class probabilities via PauliZ expectation |

- **Qubits:** 4 (one per feature for Iris) / 8 (PCA-reduced for MNIST)
- **Trainable parameters:** 24 (Iris) / 64 (MNIST)
- **Optimizer:** Adam with cross-entropy loss
- **Key insight:** 24 quantum parameters compete with ~3,000+ classical parameters thanks to the exponentially large Hilbert space (2‚Å¥ = 16 dimensions from 4 qubits)

#### How It Works ?

```
Classical Input ‚Üí [Encode into Qubits] ‚Üí [Parameterized Quantum Gates] ‚Üí [Measure] ‚Üí Prediction
                   (Angle Embedding)      (Trainable RY/RZ + CNOT)       (PauliZ)
```

1. **Encoding:** Each feature `x·µ¢` becomes a rotation angle `RX(x·µ¢)` on qubit `q·µ¢`
2. **Processing:** Variational layers apply learnable rotations and CNOT entanglement ‚Äî this is where the quantum "computation" happens
3. **Measurement:** PauliZ expectations on selected qubits produce values in [-1, +1], mapped to class probabilities via softmax

---

## Tech Stack

| Component | Technology | Role |
|-----------|-----------|------|
| **Quantum Simulation** | PennyLane (`default.qubit`) | Variational quantum circuits, automatic differentiation of quantum gates |
| **Classical Deep Learning** | PyTorch | Neural networks, hybrid quantum-classical backpropagation |
| **Classical ML** | scikit-learn | SVM, StandardScaler, MinMaxScaler, GridSearchCV, metrics |
| **Data Processing** | NumPy | Array operations, numerical computation |
| **Visualization** | Matplotlib, Seaborn | Training curves, confusion matrices, comparison charts |
| **Language** | Python 3 | All components |

---

## Setup & Installation

### Prerequisites

- Python 3.8+
- pip

### Install Dependencies

```bash
cd "QNN Formulation"
pip install -r requirements.txt
```

### Dependencies

```
numpy
pandas
scikit-learn
matplotlib
seaborn
torch
pennylane
```
---

## Results & Comparison

### Key Metrics Evaluated

- **Accuracy** (Train & Test)
- **Precision** (macro-averaged)
- **Recall** (macro-averaged)
- **F1-Score** (macro-averaged)
- **Confusion Matrix** (per model)
- **Parameter Efficiency** (accuracy per parameter)
- **Training Time**

---

## References

1. **Ventura, D., & Martinez, T.** (2003). *Training a Quantum Neural Network.* Advances in Neural Information Processing Systems (NIPS).
2. **Schuld, M., & Petruccione, F.** (2021). *Machine Learning with Quantum Computers.* Springer.
3. **PennyLane Documentation.** [pennylane.ai](https://pennylane.ai/)
4. **UCI Machine Learning Repository.** Iris & MNIST Datasets.

---

<p align="center">
  <em>Built with ‚öõÔ∏è PennyLane + üî• PyTorch + üêç scikit-learn</em>
</p>
